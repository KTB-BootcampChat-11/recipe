"""
ìŒì„± ì¸ì‹(STT) ì„œë¹„ìŠ¤ ëª¨ë“ˆ.

Whisper APIì™€ Silero VADë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""
import io
import logging
import re
import warnings

# torchaudio deprecation ê²½ê³  ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore", message=".*torchaudio.*deprecated.*")
warnings.filterwarnings("ignore", message=".*sox_effects.*deprecated.*")
warnings.filterwarnings("ignore", message=".*torchcodec.*")

from pathlib import Path
from typing import Any, Dict, List, Optional

import torch
import torchaudio
from openai import APIConnectionError, APIError, OpenAI, RateLimitError

from app.config import (
    MAX_AUDIO_FILE_SIZE,
    OPENAI_API_KEY,
    OPENAI_MODEL_WHISPER,
    SUPPORTED_AUDIO_FORMATS,
    VAD_MAX_SEGMENT_DURATION,
    VAD_MIN_SILENCE_DURATION,
    VAD_MIN_SPEECH_DURATION,
    VAD_SAMPLE_RATE,
    VAD_SPEECH_PAD_MS,
)
from app.exceptions import AudioFileError, TranscriptionError
from app.prompts import COOKING_PROMPT

# =============================================================================
# ë¡œê¹… ë° í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# =============================================================================
logger = logging.getLogger(__name__)
client = OpenAI(api_key=OPENAI_API_KEY)

# =============================================================================
# VAD ëª¨ë¸ ê´€ë¦¬ (ì‹±ê¸€í†¤)
# =============================================================================
_vad_model = None
_vad_utils = None


def _get_vad_model():
    """VAD ëª¨ë¸ ì‹±ê¸€í†¤ ë¡œë“œ (torch hub ì‚¬ìš©)."""
    global _vad_model, _vad_utils
    if _vad_model is None:
        logger.info("Silero VAD ëª¨ë¸ ë¡œë”© (torch hub)...")
        _vad_model, _vad_utils = torch.hub.load(
            repo_or_dir="snakers4/silero-vad",
            model="silero_vad",
            trust_repo=True
        )
        logger.info("Silero VAD ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    return _vad_model, _vad_utils


# =============================================================================
# í…ìŠ¤íŠ¸ ì²˜ë¦¬ íŒ¨í„´ (pre-compiled)
# =============================================================================
# ë¬¸ì¥ ì¢…ê²° íŒ¨í„´ (í•œêµ­ì–´)
SENTENCE_ENDINGS = re.compile(
    r"(ìš”|ë‹¤|ì£ |ë„¤ìš”|ì„¸ìš”|í•´ìš”|í•˜ì„¸ìš”|í•©ë‹ˆë‹¤|ë©ë‹ˆë‹¤|ì…ë‹ˆë‹¤|ìˆì–´ìš”|ì—†ì–´ìš”|"
    r"ì£¼ì„¸ìš”|ë“œì„¸ìš”|ë„£ìœ¼ì„¸ìš”|ë³¶ìœ¼ì„¸ìš”|ì°ì–´ì£¼ì„¸ìš”|êµ¬ì›Œì£¼ì„¸ìš”|ë“ì—¬ì£¼ì„¸ìš”|"
    r"ê±°ë“ ìš”|ì–ì•„ìš”|ëŒ€ìš”|ë˜ìš”|ëƒê³ ìš”|ëŠ”ë°ìš”|ì–´ìš”|ì•„ìš”|"
    r"ê³ ìš”|êµ¬ìš”|ë‚˜ìš”|ê¹Œìš”|ã„¹ê¹Œìš”|ì„ê¹Œìš”|"
    r"ë‹ˆë‹¤|ã…‚ë‹ˆë‹¤|ìŠµë‹ˆë‹¤|"
    r"ê±°ì˜ˆìš”|ê±´ë°ìš”|ì„¸ìš”|ë„¤ìš”|ì£ |ì–´|ì•¼)[\.\!\?]?$"
)

# ë¬¸ì¥ êµ¬ë¶„ì íŒ¨í„´ (ì‰¼í‘œ, ì ‘ì†ì–´ ë“±)
SENTENCE_CONNECTORS = re.compile(
    r"(,\s*|ê·¸ë¦¬ê³ \s+|ê·¸ëŸ°ë°\s+|ê·¸ë˜ì„œ\s+|ê·¸ëŸ¬ë©´\s+|ë‹¤ìŒì—\s+|ê·¸ë‹¤ìŒì—\s+|"
    r"ë¨¼ì €\s+|ì¼ë‹¨\s+|ê·¸ëŸ¬ê³ \s+ë‚˜ì„œ\s+|ì´ì œ\s+)"
)


# =============================================================================
# íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
# =============================================================================
def _validate_audio_file(audio_path: str) -> None:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬.

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

    Raises:
        AudioFileError: íŒŒì¼ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
    """
    path = Path(audio_path)

    if not path.exists():
        raise AudioFileError(
            f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}"
        )

    file_size = path.stat().st_size
    if file_size == 0:
        raise AudioFileError(f"ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {audio_path}")

    if file_size > MAX_AUDIO_FILE_SIZE:
        raise AudioFileError(
            f"ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤: {file_size / 1024 / 1024:.1f}MB "
            f"(ìµœëŒ€ {MAX_AUDIO_FILE_SIZE / 1024 / 1024}MB)"
        )

    if path.suffix.lower() not in SUPPORTED_AUDIO_FORMATS:
        raise AudioFileError(
            f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ì…ë‹ˆë‹¤: {path.suffix} "
            f"(ì§€ì› í˜•ì‹: {', '.join(SUPPORTED_AUDIO_FORMATS)})"
        )


# =============================================================================
# VAD (Voice Activity Detection)
# =============================================================================
def _detect_speech_segments(audio_path: str) -> List[Dict[str, float]]:
    """
    VADë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ì—ì„œ ë°œí™” êµ¬ê°„ì„ ê°ì§€í•©ë‹ˆë‹¤.

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

    Returns:
        ë°œí™” êµ¬ê°„ ë¦¬ìŠ¤íŠ¸ [{'start': float, 'end': float}, ...]
    """
    model, utils = _get_vad_model()
    (get_speech_timestamps, _, read_audio, _, _) = utils

    wav = read_audio(audio_path, sampling_rate=VAD_SAMPLE_RATE)

    speech_timestamps = get_speech_timestamps(
        wav,
        model,
        sampling_rate=VAD_SAMPLE_RATE,
        min_speech_duration_ms=int(VAD_MIN_SPEECH_DURATION * 1000),
        min_silence_duration_ms=int(VAD_MIN_SILENCE_DURATION * 1000),
        speech_pad_ms=VAD_SPEECH_PAD_MS,
        return_seconds=True
    )

    if not speech_timestamps:
        logger.warning("VAD: ë°œí™” êµ¬ê°„ì„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
        return []

    segments = _split_long_segments(speech_timestamps)
    logger.info(f"VAD: {len(segments)}ê°œ ë°œí™” êµ¬ê°„ ê°ì§€")
    return segments


def _split_long_segments(
    timestamps: List[Dict[str, float]]
) -> List[Dict[str, float]]:
    """
    ê¸´ ë°œí™” êµ¬ê°„ì„ ìµœëŒ€ ê¸¸ì´ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

    Args:
        timestamps: VAD íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸

    Returns:
        ë¶„í• ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    segments = []
    for ts in timestamps:
        start = ts["start"]
        end = ts["end"]
        duration = end - start

        if duration > VAD_MAX_SEGMENT_DURATION:
            current = start
            while current < end:
                seg_end = min(current + VAD_MAX_SEGMENT_DURATION, end)
                segments.append({"start": current, "end": seg_end})
                current = seg_end
        else:
            segments.append({"start": start, "end": end})

    return segments


def _extract_audio_segment(
    audio_path: str,
    start: float,
    end: float
) -> io.BytesIO:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ íŠ¹ì • êµ¬ê°„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        audio_path: ì›ë³¸ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        start: ì‹œì‘ ì‹œê°„ (ì´ˆ)
        end: ë ì‹œê°„ (ì´ˆ)

    Returns:
        WAV í˜•ì‹ì˜ ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ ë²„í¼
    """
    waveform, sample_rate = torchaudio.load(audio_path)

    start_sample = int(start * sample_rate)
    end_sample = int(end * sample_rate)
    segment = waveform[:, start_sample:end_sample]

    buffer = io.BytesIO()
    torchaudio.save(buffer, segment, sample_rate, format="wav")
    buffer.seek(0)

    return buffer


# =============================================================================
# Whisper API í˜¸ì¶œ
# =============================================================================
def _transcribe_segment(
    audio_bytes: io.BytesIO,
    language: str = "ko"
) -> Any:
    """
    ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ Whisper APIë¡œ ì „ì‚¬í•©ë‹ˆë‹¤.

    Args:
        audio_bytes: ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ ë²„í¼
        language: ì–¸ì–´ ì½”ë“œ

    Returns:
        Whisper API ì‘ë‹µ
    """
    audio_bytes.name = "segment.wav"

    response = client.audio.transcriptions.create(
        model=OPENAI_MODEL_WHISPER,
        file=audio_bytes,
        response_format="verbose_json",
        timestamp_granularities=["word", "segment"],
        language=language,
        prompt=COOKING_PROMPT
    )

    return response


# =============================================================================
# í…ìŠ¤íŠ¸ ì •ì œ
# =============================================================================
def _clean_transcript_text(text: str) -> str:
    """
    ì „ì‚¬ í…ìŠ¤íŠ¸ ì •ì œ (ë°˜ë³µ, í•„ëŸ¬ ë‹¨ì–´, ì˜¤ì¸ì‹ íŒ¨í„´ ì œê±°).

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸

    Returns:
        ì •ì œëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return ""

    # í•„ëŸ¬ ë‹¨ì–´ ì œê±°
    text = re.sub(r"\b(ìŒ+|ì–´+|ê·¸+|ì•„+|ì—+)\.{0,3}\s*", "", text)

    # ë°˜ë³µë˜ëŠ” ê°íƒ„ì‚¬ ì œê±°
    text = re.sub(r"\b(ë„¤ë„¤|ì•„ì•„|ì˜¤ì˜¤|ì™€ì™€|ìŒìŒ|ì–´ì–´)\b", "", text)

    # ì˜ë¯¸ì—†ëŠ” ë°˜ë³µ íŒ¨í„´ ì œê±°
    text = re.sub(r"\b(\w+)(\s+\1){2,}\b", r"\1", text)

    # Whisper ì˜¤ì¸ì‹ íŒ¨í„´ êµì • (ìœ íŠœë¸Œ ê´€ë ¨)
    text = re.sub(r"êµ¬ë…\s*ì¢‹ì•„ìš”\s*ì•Œë¦¼", "", text)
    text = re.sub(r"êµ¬ë…ê³¼\s*ì¢‹ì•„ìš”", "", text)

    # ë°°ê²½ìŒì•… ì¸ì‹ ì˜¤ë¥˜ ì œê±°
    text = re.sub(r"â™ª+|â™«+|ğŸµ+|ğŸ¶+", "", text)
    text = re.sub(
        r"\[ìŒì•…\]|\[ë°°ê²½ìŒì•…\]|\[BGM\]",
        "",
        text,
        flags=re.IGNORECASE
    )

    # ìˆ«ì+ë‹¨ìœ„ ì •ê·œí™”
    text = re.sub(r"(\d+)\s*ìŠ¤í‘¼", r"\1ìŠ¤í‘¼", text)
    text = re.sub(r"(\d+)\s*í°ìˆ ", r"\1í°ìˆ ", text)
    text = re.sub(r"(\d+)\s*ì‘ì€ìˆ ", r"\1ì‘ì€ìˆ ", text)
    text = re.sub(r"(\d+)\s*ë¶„", r"\1ë¶„", text)
    text = re.sub(r"(\d+)\s*ì´ˆ", r"\1ì´ˆ", text)
    text = re.sub(r"(\d+)\s*ê·¸ë¨", r"\1g", text)
    text = re.sub(r"(\d+)\s*g", r"\1g", text)
    text = re.sub(r"(\d+)\s*ml", r"\1ml", text, flags=re.IGNORECASE)

    # ì—°ì†ëœ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r"\s+", " ", text)

    # ë¬¸ì¥ ë¶€í˜¸ ì •ë¦¬
    text = re.sub(r"\.{2,}", ".", text)
    text = re.sub(r"\s+([,.!?])", r"\1", text)

    return text.strip()


# =============================================================================
# ë¬¸ì¥ ë¶„ë¦¬
# =============================================================================
def _split_into_sentences(response: Any) -> List[Dict[str, Any]]:
    """
    Word íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.

    Args:
        response: Whisper API ì‘ë‹µ

    Returns:
        ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    words = getattr(response, "words", None)

    if not words:
        return _extract_segments_from_response(response)

    sentences = []
    current_words = []
    current_start = None

    for i, word in enumerate(words):
        word_text = getattr(word, "word", "").strip()
        word_start = getattr(word, "start", 0)
        word_end = getattr(word, "end", 0)

        if not word_text:
            continue

        if current_start is None:
            current_start = word_start

        current_words.append(word_text)

        is_sentence_end = _check_sentence_end(
            word_text, word_end, words, i, len(current_words)
        )

        if is_sentence_end:
            sentence_text = _join_words(current_words)
            if sentence_text:
                sentences.append({
                    "start": round(current_start, 2),
                    "end": round(word_end, 2),
                    "text": sentence_text
                })
            current_words = []
            current_start = None

    # ë‚¨ì€ ë‹¨ì–´ë“¤ ì²˜ë¦¬
    if current_words:
        sentence_text = _join_words(current_words)
        if sentence_text and words:
            sentences.append({
                "start": round(current_start or 0, 2),
                "end": round(getattr(words[-1], "end", 0), 2),
                "text": sentence_text
            })

    if not sentences:
        return _extract_segments_from_response(response)

    return sentences


def _check_sentence_end(
    word_text: str,
    word_end: float,
    words: List[Any],
    index: int,
    word_count: int
) -> bool:
    """
    ë¬¸ì¥ ì¢…ê²° ì¡°ê±´ì„ ì²´í¬í•©ë‹ˆë‹¤.

    Args:
        word_text: í˜„ì¬ ë‹¨ì–´ í…ìŠ¤íŠ¸
        word_end: í˜„ì¬ ë‹¨ì–´ ì¢…ë£Œ ì‹œê°„
        words: ì „ì²´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
        index: í˜„ì¬ ì¸ë±ìŠ¤
        word_count: í˜„ì¬ê¹Œì§€ ëˆ„ì  ë‹¨ì–´ ìˆ˜

    Returns:
        ë¬¸ì¥ ì¢…ê²° ì—¬ë¶€
    """
    # ë¬¸ì¥ ì¢…ê²° íŒ¨í„´ ë§¤ì¹­
    if SENTENCE_ENDINGS.search(word_text):
        return True

    # ê¸´ pause ê°ì§€ (0.8ì´ˆ ì´ìƒ)
    if index < len(words) - 1:
        next_start = getattr(words[index + 1], "start", 0)
        if next_start - word_end > 0.8:
            return True

    # ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´ ê°•ì œ ë¶„ë¦¬ (15ë‹¨ì–´ ì´ìƒ)
    if word_count >= 15:
        return True

    return False


def _join_words(words: List[str]) -> str:
    """
    ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.

    Args:
        words: ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì—°ê²°ëœ ë¬¸ì¥
    """
    if not words:
        return ""

    text = " ".join(words)

    # ì¡°ì‚¬ ì• ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(
        r"\s+(ì„|ë¥¼|ì´|ê°€|ì€|ëŠ”|ì—|ì˜|ë¡œ|ìœ¼ë¡œ|ì™€|ê³¼|ë„|ë§Œ|ê¹Œì§€|ë¶€í„°|ì—ì„œ)\b",
        r"\1",
        text
    )

    # ìˆ«ìì™€ ë‹¨ìœ„ ì‚¬ì´ ê³µë°± ì œê±°
    text = re.sub(
        r"(\d+)\s+(ë¶„|ì´ˆ|g|ml|ê°œ|ì¥|í°ìˆ |ì‘ì€ìˆ |ìŠ¤í‘¼|ì»µ)",
        r"\1\2",
        text
    )

    return text.strip()


def _extract_segments_from_response(response: Any) -> List[Dict[str, Any]]:
    """
    Whisper ì‘ë‹µì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        response: Whisper API ì‘ë‹µ

    Returns:
        ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    segments = []

    if hasattr(response, "segments") and response.segments:
        for segment in response.segments:
            text = getattr(segment, "text", "").strip()
            if text:
                segments.append({
                    "start": getattr(segment, "start", 0),
                    "end": getattr(segment, "end", 0),
                    "text": text
                })

    return segments


# =============================================================================
# ë©”ì¸ ì „ì‚¬ í•¨ìˆ˜
# =============================================================================
async def transcribe_audio(
    audio_path: str,
    language: str = "ko",
    use_vad: bool = True
) -> Dict[str, Any]:
    """
    VAD + Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        language: ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸: ko)
        use_vad: VAD ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True)

    Returns:
        ì „ì‚¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬:
        - full_text: ì „ì²´ í…ìŠ¤íŠ¸
        - segments: ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        - language: ì–¸ì–´ ì½”ë“œ
        - duration: ì˜¤ë””ì˜¤ ê¸¸ì´

    Raises:
        TranscriptionError: ìŒì„± ì¸ì‹ ì‹¤íŒ¨ ì‹œ
        AudioFileError: ì˜¤ë””ì˜¤ íŒŒì¼ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
    """
    _validate_audio_file(audio_path)

    logger.info(f"ìŒì„± ì¸ì‹ ì‹œì‘: {audio_path} (VAD: {use_vad})")

    try:
        if use_vad:
            return await _transcribe_with_vad(audio_path, language)
        else:
            return await _transcribe_full_audio(audio_path, language)

    except (RateLimitError, APIConnectionError, APIError) as e:
        logger.error(f"API ì˜¤ë¥˜: {e}")
        raise TranscriptionError(f"ìŒì„± ì¸ì‹ API ì˜¤ë¥˜: {e}")

    except AudioFileError:
        raise

    except Exception as e:
        logger.error(f"ìŒì„± ì¸ì‹ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        raise TranscriptionError(f"ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


async def _transcribe_with_vad(
    audio_path: str,
    language: str
) -> Dict[str, Any]:
    """VADë¥¼ ì‚¬ìš©í•˜ì—¬ ë°œí™” êµ¬ê°„ë³„ë¡œ ì „ì‚¬í•©ë‹ˆë‹¤."""
    speech_segments = _detect_speech_segments(audio_path)

    if not speech_segments:
        logger.warning("VAD: ë°œí™” êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ì˜¤ë””ì˜¤ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
        return await _transcribe_full_audio(audio_path, language)

    all_segments = []
    all_texts = []
    total_duration = 0

    for i, seg in enumerate(speech_segments):
        logger.info(
            f"VAD êµ¬ê°„ {i + 1}/{len(speech_segments)}: "
            f"{seg['start']:.1f}s - {seg['end']:.1f}s"
        )

        audio_buffer = _extract_audio_segment(
            audio_path, seg["start"], seg["end"]
        )
        response = _transcribe_segment(audio_buffer, language)

        segment_text = getattr(response, "text", "") or ""

        if segment_text.strip():
            cleaned_text = _clean_transcript_text(segment_text)
            all_texts.append(cleaned_text)

            sub_segments = _split_into_sentences(response)

            # ì›ë³¸ ì˜¤ë””ì˜¤ ê¸°ì¤€ìœ¼ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ ì˜¤í”„ì…‹ ì ìš©
            for sub_seg in sub_segments:
                sub_seg["start"] = round(seg["start"] + sub_seg["start"], 2)
                sub_seg["end"] = round(seg["start"] + sub_seg["end"], 2)
                sub_seg["text"] = _clean_transcript_text(sub_seg["text"])

            all_segments.extend(sub_segments)

        total_duration = max(total_duration, seg["end"])

    full_text = " ".join(all_texts)

    logger.info(
        f"ìŒì„± ì¸ì‹ ì™„ë£Œ (VAD): {len(full_text)}ì, "
        f"{len(all_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸"
    )

    return {
        "full_text": full_text,
        "segments": all_segments,
        "language": language,
        "duration": total_duration
    }


async def _transcribe_full_audio(
    audio_path: str,
    language: str
) -> Dict[str, Any]:
    """ì „ì²´ ì˜¤ë””ì˜¤ë¥¼ í•œ ë²ˆì— ì „ì‚¬í•©ë‹ˆë‹¤ (VAD ë¯¸ì‚¬ìš© í´ë°±)."""
    with open(audio_path, "rb") as audio_file:
        response = client.audio.transcriptions.create(
            model=OPENAI_MODEL_WHISPER,
            file=audio_file,
            response_format="verbose_json",
            timestamp_granularities=["word", "segment"],
            language=language,
            prompt=COOKING_PROMPT
        )

    if not response:
        raise TranscriptionError("ìŒì„± ì¸ì‹ ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

    full_text = getattr(response, "text", "") or ""

    if not full_text.strip():
        logger.warning("ìŒì„± ì¸ì‹ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤ (ë¬´ìŒ ë˜ëŠ” ì¸ì‹ ì‹¤íŒ¨)")
        return {
            "full_text": "",
            "segments": [],
            "language": language,
            "duration": getattr(response, "duration", 0)
        }

    cleaned_text = _clean_transcript_text(full_text)
    segments = _split_into_sentences(response)

    for seg in segments:
        seg["text"] = _clean_transcript_text(seg["text"])

    logger.info(
        f"ìŒì„± ì¸ì‹ ì™„ë£Œ: {len(cleaned_text)}ì, {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸"
    )

    return {
        "full_text": cleaned_text,
        "segments": segments,
        "language": getattr(response, "language", language),
        "duration": getattr(response, "duration", 0)
    }
